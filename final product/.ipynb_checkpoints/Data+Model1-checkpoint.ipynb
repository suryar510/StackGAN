{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import PIL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def load(classes, embeddings, filenames):\n",
    "    with open(classes, 'rb') as f:\n",
    "        class_ids = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    with open(embeddings, 'rb') as f:\n",
    "        embeddings = pickle.load(f, encoding='latin1')\n",
    "        embeddings = np.array(embeddings)\n",
    "\n",
    "    with open(filenames, 'rb') as f:\n",
    "        filenames = pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    return class_ids, embeddings, filenames\n",
    "\n",
    "def load_bounding_boxes(dataset_dir):\n",
    "    bounding_boxes_path = os.path.join(dataset_dir, 'bounding_boxes.txt')\n",
    "    file_paths_path = os.path.join(dataset_dir, 'images.txt')\n",
    "\n",
    "    df_bounding_boxes = pd.read_csv(bounding_boxes_path,\n",
    "                                    delim_whitespace=True, header=None).astype(int)\n",
    "    df_file_names = pd.read_csv(file_paths_path, delim_whitespace=True, header=None)\n",
    "\n",
    "    # Create a list of file names\n",
    "    file_names = df_file_names[1].tolist()\n",
    "\n",
    "    # Create a dictionary of file_names and bounding boxes\n",
    "    filename_boundingbox_dict = {img_file[:-4]: [] for img_file in file_names[:2]}\n",
    "\n",
    "    # Assign a bounding box to the corresponding image\n",
    "    for i in range(0, len(file_names)):\n",
    "        # Get the bounding box\n",
    "        bounding_box = df_bounding_boxes.iloc[i][1:].tolist()\n",
    "        key = file_names[i][:-4]\n",
    "        filename_boundingbox_dict[key] = bounding_box\n",
    "\n",
    "    return filename_boundingbox_dict\n",
    "\n",
    "def get_img(img_path, bbox, image_size):\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    width, height = img.size\n",
    "    if bbox is not None:\n",
    "        R = int(np.maximum(bbox[2], bbox[3]) * 0.75)\n",
    "        center_x = int((2 * bbox[0] + bbox[2]) / 2)\n",
    "        center_y = int((2 * bbox[1] + bbox[3]) / 2)\n",
    "        y1 = np.maximum(0, center_y - R)\n",
    "        y2 = np.minimum(height, center_y + R)\n",
    "        x1 = np.maximum(0, center_x - R)\n",
    "        x2 = np.minimum(width, center_x + R)\n",
    "        img = img.crop([x1, y1, x2, y2])\n",
    "    img = img.resize(image_size, PIL.Image.BILINEAR)\n",
    "    return img\n",
    "\n",
    "def load_dataset(files, classes, embeds, cub, size):\n",
    "    class_ids, filed_embeddings, filenames = load(classes, embeds, files)\n",
    "    bounding_boxes = load_bounding_boxes(cub)\n",
    "    X, y, embeddings = [], [], []\n",
    "\n",
    "    for index, filename in enumerate(filenames):\n",
    "        box = bounding_boxes[filename]\n",
    "\n",
    "        try:\n",
    "            # Load images\n",
    "            name = cub + '/images/' + filename + '.jpg'\n",
    "            image = get_img(name, box, size)\n",
    "\n",
    "            all_embeddings = filed_embeddings[index, :, :]\n",
    "            embedding_ix = random.randint(0, all_embeddings.shape[0] - 1)\n",
    "            embedding = all_embeddings[embedding_ix, :]\n",
    "\n",
    "            X.append(np.array(image))\n",
    "            y.append(class_ids[index])\n",
    "            embeddings.append(embedding)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    X_data = np.asarray(X, dtype=np.float32)\n",
    "    y_data = np.asarray(y, dtype=np.float32)\n",
    "    embeddings = np.asarray(embeddings, dtype=np.float32)\n",
    "    return X_data, y_data, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3(x, out):\n",
    "    x = Conv2D(out, kernel_size=3, strides=1, padding='same', use_bias=False)(x)\n",
    "    return x\n",
    "\n",
    "def upblock(x, out):\n",
    "    x = UpSampling2D((2,2), interpolation='nearest')(x)\n",
    "    x = conv3(x, out)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    return x\n",
    "\n",
    "def downblock(x, out):\n",
    "    x = Conv2D(out, (4, 4), padding='same', strides=2, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    return x\n",
    "\n",
    "def build_ca():\n",
    "    embed = Input(shape=(1024,))\n",
    "    x = Dense(256)(embed)\n",
    "    mulogsigma = LeakyReLU(alpha=0.2)(x)\n",
    "    ca = Model(inputs=[embed], outputs=[mulogsigma])\n",
    "    return ca\n",
    "\n",
    "def build_compembed():\n",
    "    embed = Input(shape=(1024,))\n",
    "    x = Dense(128)(embed)\n",
    "    x = ReLU()(x)\n",
    "    compembed = Model(inputs=[embed], outputs=[x])\n",
    "    return compembed\n",
    "\n",
    "def samplec(mulogsigma):\n",
    "    mu = mulogsigma[:, :128]\n",
    "    logsigma = mulogsigma[:, 128:]\n",
    "    stddev = K.exp(logsigma)\n",
    "    epsilon = K.random_normal(shape=K.constant((mu.shape[1],), dtype='int32'))\n",
    "    cond = stddev * epsilon + mu\n",
    "    return cond\n",
    "\n",
    "def build_gen():\n",
    "    embed = Input(shape=(1024,))\n",
    "    x = Dense(256)(embed)\n",
    "    mulogsigma = LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    cond = Lambda(samplec)(mulogsigma)\n",
    "    noise = Input(shape=(100,))\n",
    "    x = Concatenate(axis=1)([cond, noise])\n",
    "\n",
    "    x = Dense(128 * 8 * 4 * 4, use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Reshape((4, 4, 128 * 8), input_shape=(128 * 8 * 4 * 4,))(x)\n",
    "\n",
    "    x = upblock(x, 512)\n",
    "    x = upblock(x, 256)\n",
    "    x = upblock(x, 128)\n",
    "    x = upblock(x, 64)\n",
    "\n",
    "    x = conv3(x, 3)\n",
    "    image = Activation(activation='tanh')(x)\n",
    "\n",
    "    gen = Model(inputs=[embed, noise], outputs=[image, mulogsigma])\n",
    "    return gen\n",
    "\n",
    "def build_dis():\n",
    "    image = Input(shape=(64, 64, 3))\n",
    "    x = Conv2D(64, (4, 4), padding='same', strides=2, input_shape=(64, 64, 3), use_bias=False)(image)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = downblock(x, 128)\n",
    "    x = downblock(x, 256)\n",
    "    x = downblock(x, 512)\n",
    "\n",
    "    compembed = Input(shape=(4, 4, 128))\n",
    "    x = concatenate([x, compembed])\n",
    "    x = Conv2D(64 * 8, kernel_size=1, padding='same', strides=1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "\n",
    "    dis = Model(inputs=[image, compembed], outputs=[x])\n",
    "    return dis\n",
    "\n",
    "def build_gan(gen, dis):\n",
    "    embed = Input(shape=(1024,))\n",
    "    noise = Input(shape=(100,))\n",
    "    compembed = Input(shape=(4, 4, 128))\n",
    "\n",
    "    image, mulogsigma = gen([embed, noise])\n",
    "\n",
    "    dis.trainable = False\n",
    "    score = dis([image, compembed])\n",
    "\n",
    "    gan = Model(inputs=[embed, noise, compembed], outputs=[score, mulogsigma])\n",
    "    return gan\n",
    "\n",
    "def train():\n",
    "    data_dir = \"/Users/alexnails/Desktop/StackGAN/birds\"\n",
    "    train_dir = data_dir + \"/train\"\n",
    "    test_dir = data_dir + \"/test\"\n",
    "    image_size = 64\n",
    "    batch_size = 64\n",
    "    noise_dim = 100\n",
    "    gen_lr = 0.0002\n",
    "    dis_lr = 0.0002\n",
    "    lr_decay_step = 600\n",
    "    epochs = 1000\n",
    "    cond_dim = 128\n",
    "\n",
    "    embeddings_file_path_train = train_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "    embeddings_file_path_test = test_dir + \"/char-CNN-RNN-embeddings.pickle\"\n",
    "\n",
    "    filenames_file_path_train = train_dir + \"/filenames.pickle\"\n",
    "    filenames_file_path_test = test_dir + \"/filenames.pickle\"\n",
    "\n",
    "    class_info_file_path_train = train_dir + \"/class_info.pickle\"\n",
    "    class_info_file_path_test = test_dir + \"/class_info.pickle\"\n",
    "\n",
    "    cub_dataset_dir = \"/Users/alexnails/Desktop/StackGAN/CUB_200_2011\"\n",
    "\n",
    "    print(\"Loading Training\")\n",
    "    X_train, y_train, embed_train = load_dataset(files=filenames_file_path_train,\n",
    "                                                      classes=class_info_file_path_train,\n",
    "                                                 embeds=embeddings_file_path_train,\n",
    "                                                 cub=cub_dataset_dir,\n",
    "                                                 size=(64,64))\n",
    "    print (\"Loading Testing\")\n",
    "    X_test, y_test, embed_test =  load_dataset(files=filenames_file_path_train,\n",
    "                                                      classes=class_info_file_path_train,\n",
    "                                                 embeds=embeddings_file_path_train,\n",
    "                                                 cub=cub_dataset_dir,\n",
    "                                                 size=(64,64))\n",
    "\n",
    "    ca = build_ca()\n",
    "    ca.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    compembed = build_compembed()\n",
    "    compembed.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "    dis = build_dis()\n",
    "    dis_optimizer = Adam(lr=dis_lr, beta_1=0.5, beta_2=0.999)\n",
    "    dis.compile(loss='binary_crossentropy', optimizer=dis_optimizer)\n",
    "\n",
    "    gen = build_gen()\n",
    "    gen_optimizer = Adam(lr=gen_lr, beta_1=0.5, beta_2=0.999)\n",
    "    gen.compile(loss=\"mse\", optimizer=gen_optimizer)\n",
    "\n",
    "    def KL_loss(y_true, y_pred):\n",
    "        mu = y_pred[:, :128]\n",
    "        logsigma = y_pred[:, :128]\n",
    "        loss = -logsigma + .5 * (-1 + K.exp(2. * logsigma) + K.square(mu))\n",
    "        loss = K.mean(loss)\n",
    "        return loss\n",
    "\n",
    "    gan = build_gan(gen, dis)\n",
    "    gan.compile(loss=['binary_crossentropy', KL_loss], loss_weights=[1,2.0],\n",
    "                              optimizer=gen_optimizer, metrics=None)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"logs/\".format(time.time()))\n",
    "    tensorboard.set_model(gen)\n",
    "    tensorboard.set_model(dis)\n",
    "    tensorboard.set_model(ca)\n",
    "    tensorboard.set_model(compembed)\n",
    "\n",
    "    # Generate an array containing real and fake values\n",
    "    # Apply label smoothing as well\n",
    "    real_labels = np.ones((batch_size, 1), dtype=float) * 0.9\n",
    "    fake_labels = np.zeros((batch_size, 1), dtype=float) * 0.1\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"========================================\")\n",
    "        print(\"Epoch:\", epoch)\n",
    "        print(\"Number of batches:\", int(X_train.shape[0] / batch_size))\n",
    "\n",
    "        gen_losses = []\n",
    "        dis_losses = []\n",
    "\n",
    "        # Load and train\n",
    "        num_batches = int(X_train.shape[0] / batch_size)\n",
    "        for index in range(num_batches):\n",
    "            print(\"Batch:{}\".format(index+1))\n",
    "\n",
    "            # Sample a batch of data\n",
    "            noise_batch = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "            image_batch = X_train[index * batch_size:(index + 1) * batch_size]\n",
    "            image_batch = (image_batch - 127.5) / 127.5\n",
    "            embed_batch = embed_train[index * batch_size:(index + 1) * batch_size]\n",
    "\n",
    "            # Generate fake images\n",
    "            fake_images, _ = gen.predict([embed_batch, noise_batch], verbose=3)\n",
    "            \n",
    "            # Generate compressed embeddings\n",
    "            compembed_batch = compembed.predict_on_batch(embed_batch)\n",
    "            compembed_batch = np.reshape(compembed_batch, (-1, 1, 1, cond_dim))\n",
    "            compembed_batch = np.tile(compembed_batch, (1, 4, 4, 1))\n",
    "\n",
    "            dis_loss_real = dis.train_on_batch([image_batch, compembed_batch],\n",
    "                                                      np.reshape(real_labels, (batch_size, 1)))\n",
    "            dis_loss_fake = dis.train_on_batch([fake_images, compembed_batch],\n",
    "                                                      np.reshape(fake_labels, (batch_size, 1)))\n",
    "            dis_loss_wrong = dis.train_on_batch([image_batch[:(batch_size - 1)], compembed_batch[1:]],\n",
    "                                                       np.reshape(fake_labels[1:], (batch_size-1, 1)))\n",
    "            dis_loss = 0.5 * np.add(dis_loss_real, 0.5 * np.add(dis_loss_wrong, dis_loss_fake))\n",
    "            print(\"d_loss_real:{}\".format(dis_loss_real))\n",
    "            print(\"d_loss_fake:{}\".format(dis_loss_fake))\n",
    "            print(\"d_loss_wrong:{}\".format(dis_loss_wrong))\n",
    "            print(\"dis_loss:{}\".format(dis_loss))\n",
    "\n",
    "            gen_loss = gan.train_on_batch([embed_batch, noise_batch, compembed_batch],[K.ones((batch_size, 1)) * 0.9, K.ones((batch_size, 256)) * 0.9])\n",
    "            print(\"gen_loss:{}\".format(gen_loss))\n",
    "\n",
    "            dis_losses.append(dis_loss)\n",
    "            gen_losses.append(gen_loss)\n",
    "\n",
    "        write_log(tensorboard, 'dis_loss', np.mean(dis_losses), epoch)\n",
    "        write_log(tensorboard, 'gen_loss', np.mean(gen_losses[0]), epoch)\n",
    "\n",
    "        # Generate and save images after every 2nd epoch\n",
    "        #if epoch % 2 == 0:\n",
    "            # z_noise2 = np.random.uniform(-1, 1, size=(batch_size, noise_dim))\n",
    "        noise_batch = np.random.normal(0, 1, size=(batch_size, noise_dim))\n",
    "        embed_batch = embed_test[0:batch_size]\n",
    "        fake_images, _ = gen.predict_on_batch([embed_batch, noise_batch])\n",
    "\n",
    "        # Save images\n",
    "        for i, img in enumerate(fake_images[:10]):\n",
    "            save_rgb_img(img, \"/Users/alexnails/Desktop/StackGAN/results/gen_{}_{}.png\".format(epoch, i))\n",
    "\n",
    "    # Save models\n",
    "    gen.save_weights(\"stage1_gen.h5\")\n",
    "    dis.save_weights(\"stage1_dis.h5\")\n",
    "\n",
    "def save_rgb_img(image, path):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Image\")\n",
    "    plt.savefig(path)\n",
    "    plt.close()\n",
    "\n",
    "#good for tf2.0+\n",
    "def write_log(callback, name, loss, batch_no):\n",
    "    writer = tf.summary.create_file_writer(\"/Users/alexnails/Desktop/StackGAN/logs/train\")\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar(name, loss, step=batch_no)\n",
    "        writer.flush()  \n",
    "\n",
    "    \n",
    "    \n",
    "#bad for tf2.0+, good for tf1\n",
    "def write_logt1(callback, name, loss, batch_no):\n",
    "    summary = tf.Summary()\n",
    "    summary_value = summary.value.add()\n",
    "    summary_value.simple_value = loss\n",
    "    summary_value.tag = name\n",
    "    callback.writer.add_summary(summary, batch_no)\n",
    "    callback.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Training\n",
      "Loading Testing\n",
      "========================================\n",
      "Epoch: 0\n",
      "Number of batches: 138\n",
      "Batch:1\n",
      "d_loss_real:0.6495885848999023\n",
      "d_loss_fake:3.8583858013153076\n",
      "d_loss_wrong:4.717846870422363\n",
      "dis_loss:2.4688525199890137\n",
      "gen_loss:[0.7377123, 0.7016363, 0.018037993]\n",
      "Batch:2\n",
      "d_loss_real:1.6454771757125854\n",
      "d_loss_fake:0.9902834892272949\n",
      "d_loss_wrong:1.1915428638458252\n",
      "dis_loss:1.3681951761245728\n",
      "gen_loss:[0.7729963, 0.72117305, 0.025911616]\n",
      "Batch:3\n",
      "d_loss_real:3.213937282562256\n",
      "d_loss_fake:0.16074982285499573\n",
      "d_loss_wrong:1.047437071800232\n",
      "dis_loss:1.9090153574943542\n",
      "gen_loss:[0.7609834, 0.7178062, 0.021588588]\n",
      "Batch:4\n",
      "d_loss_real:2.5004117488861084\n",
      "d_loss_fake:0.45054835081100464\n",
      "d_loss_wrong:1.9349833726882935\n",
      "dis_loss:1.8465887904167175\n",
      "gen_loss:[0.7839066, 0.7332716, 0.02531749]\n",
      "Batch:5\n",
      "d_loss_real:1.9379148483276367\n",
      "d_loss_fake:0.001621509320102632\n",
      "d_loss_wrong:2.158963918685913\n",
      "dis_loss:1.509103775024414\n",
      "gen_loss:[0.7460543, 0.7140272, 0.016013544]\n",
      "Batch:6\n",
      "d_loss_real:2.2013344764709473\n",
      "d_loss_fake:0.3333030939102173\n",
      "d_loss_wrong:1.3700060844421387\n",
      "dis_loss:1.5264945328235626\n",
      "gen_loss:[0.7506888, 0.7165888, 0.017049987]\n",
      "Batch:7\n",
      "d_loss_real:2.4919450283050537\n",
      "d_loss_fake:0.30357611179351807\n",
      "d_loss_wrong:0.8091706037521362\n",
      "dis_loss:1.5241591930389404\n",
      "gen_loss:[0.7535523, 0.728518, 0.012517152]\n",
      "Batch:8\n",
      "d_loss_real:2.6716179847717285\n",
      "d_loss_fake:0.0441727451980114\n",
      "d_loss_wrong:0.5240901112556458\n",
      "dis_loss:1.4778747111558914\n",
      "gen_loss:[0.71784204, 0.6763218, 0.020760119]\n",
      "Batch:9\n",
      "d_loss_real:1.9977704286575317\n",
      "d_loss_fake:0.2999441623687744\n",
      "d_loss_wrong:1.1175124645233154\n",
      "dis_loss:1.3532493710517883\n",
      "gen_loss:[0.80153006, 0.7298129, 0.03585856]\n",
      "Batch:10\n",
      "d_loss_real:1.5759894847869873\n",
      "d_loss_fake:0.03962162882089615\n",
      "d_loss_wrong:0.9201300144195557\n",
      "dis_loss:1.027932658791542\n",
      "gen_loss:[0.77715987, 0.7400063, 0.018576778]\n",
      "Batch:11\n",
      "d_loss_real:1.491917371749878\n",
      "d_loss_fake:0.0404706746339798\n",
      "d_loss_wrong:0.988563060760498\n",
      "dis_loss:1.0032171308994293\n",
      "gen_loss:[0.7331892, 0.70993245, 0.011628391]\n",
      "Batch:12\n",
      "d_loss_real:1.5082082748413086\n",
      "d_loss_fake:0.06871306896209717\n",
      "d_loss_wrong:0.9076377749443054\n",
      "dis_loss:0.9981918483972549\n",
      "gen_loss:[0.7534184, 0.7314995, 0.010959446]\n",
      "Batch:13\n",
      "d_loss_real:1.372577428817749\n",
      "d_loss_fake:0.02029978670179844\n",
      "d_loss_wrong:0.9679670929908752\n",
      "dis_loss:0.9333554357290268\n",
      "gen_loss:[0.7480273, 0.7306116, 0.00870786]\n",
      "Batch:14\n",
      "d_loss_real:1.2676795721054077\n",
      "d_loss_fake:0.04503253847360611\n",
      "d_loss_wrong:0.9969675540924072\n",
      "dis_loss:0.8943397998809814\n",
      "gen_loss:[0.7343206, 0.7191913, 0.0075646387]\n",
      "Batch:15\n",
      "d_loss_real:1.1604804992675781\n",
      "d_loss_fake:0.044406455010175705\n",
      "d_loss_wrong:1.3437451124191284\n",
      "dis_loss:0.9272781312465668\n",
      "gen_loss:[0.760359, 0.7340061, 0.01317643]\n",
      "Batch:16\n",
      "d_loss_real:1.1323844194412231\n",
      "d_loss_fake:0.029724299907684326\n",
      "d_loss_wrong:1.0194824934005737\n",
      "dis_loss:0.8284938931465149\n",
      "gen_loss:[0.72597873, 0.70419085, 0.010893937]\n",
      "Batch:17\n",
      "d_loss_real:1.380150318145752\n",
      "d_loss_fake:0.054179951548576355\n",
      "d_loss_wrong:1.5290703773498535\n",
      "dis_loss:1.0858877301216125\n",
      "gen_loss:[0.722111, 0.7099973, 0.006056838]\n",
      "Batch:18\n",
      "d_loss_real:1.3913065195083618\n",
      "d_loss_fake:0.09458303451538086\n",
      "d_loss_wrong:0.6714761853218079\n",
      "dis_loss:0.8871680647134781\n",
      "gen_loss:[0.7385791, 0.72801816, 0.0052804537]\n",
      "Batch:19\n",
      "d_loss_real:1.5422331094741821\n",
      "d_loss_fake:0.025496292859315872\n",
      "d_loss_wrong:0.8718023896217346\n",
      "dis_loss:0.9954412281513214\n",
      "gen_loss:[0.7191961, 0.7081456, 0.0055252267]\n",
      "Batch:20\n",
      "d_loss_real:1.3185960054397583\n",
      "d_loss_fake:0.08610542863607407\n",
      "d_loss_wrong:0.9791619181632996\n",
      "dis_loss:0.9256148338317871\n",
      "gen_loss:[0.73843503, 0.7273972, 0.005518906]\n",
      "Batch:21\n",
      "d_loss_real:1.5964348316192627\n",
      "d_loss_fake:0.03982264921069145\n",
      "d_loss_wrong:0.5808947682380676\n",
      "dis_loss:0.9533967673778534\n",
      "gen_loss:[0.71211016, 0.6986302, 0.006739971]\n",
      "Batch:22\n",
      "d_loss_real:1.118795394897461\n",
      "d_loss_fake:0.06136978417634964\n",
      "d_loss_wrong:0.9984011054039001\n",
      "dis_loss:0.824340432882309\n",
      "gen_loss:[0.7084918, 0.69902086, 0.004735466]\n",
      "Batch:23\n",
      "d_loss_real:1.1198718547821045\n",
      "d_loss_fake:0.023616135120391846\n",
      "d_loss_wrong:0.809688925743103\n",
      "dis_loss:0.768262192606926\n",
      "gen_loss:[0.7088357, 0.69882756, 0.0050040884]\n",
      "Batch:24\n",
      "d_loss_real:1.0306282043457031\n",
      "d_loss_fake:0.052804794162511826\n",
      "d_loss_wrong:0.888690710067749\n",
      "dis_loss:0.7506879717111588\n",
      "gen_loss:[0.73792696, 0.72459733, 0.0066648144]\n",
      "Batch:25\n",
      "d_loss_real:1.1447712182998657\n",
      "d_loss_fake:0.02278110757470131\n",
      "d_loss_wrong:0.9558573365211487\n",
      "dis_loss:0.8170452266931534\n",
      "gen_loss:[0.71728843, 0.70692277, 0.005182839]\n",
      "Batch:26\n",
      "d_loss_real:1.3436050415039062\n",
      "d_loss_fake:0.08763671666383743\n",
      "d_loss_wrong:0.7225939035415649\n",
      "dis_loss:0.8743601739406586\n",
      "gen_loss:[0.72669643, 0.71655893, 0.0050687445]\n",
      "Batch:27\n",
      "d_loss_real:1.117276668548584\n",
      "d_loss_fake:0.0071332016959786415\n",
      "d_loss_wrong:1.1345711946487427\n",
      "dis_loss:0.8440644443035126\n",
      "gen_loss:[0.707096, 0.69654924, 0.0052733603]\n",
      "Batch:28\n",
      "d_loss_real:1.1567749977111816\n",
      "d_loss_fake:0.041580911725759506\n",
      "d_loss_wrong:0.7680573463439941\n",
      "dis_loss:0.7807970643043518\n",
      "gen_loss:[0.7408525, 0.72561085, 0.0076208217]\n",
      "Batch:29\n",
      "d_loss_real:1.0903105735778809\n",
      "d_loss_fake:0.011218070983886719\n",
      "d_loss_wrong:0.7985514998435974\n",
      "dis_loss:0.7475976794958115\n",
      "gen_loss:[0.7069106, 0.694841, 0.0060347877]\n",
      "Batch:30\n",
      "d_loss_real:1.0842854976654053\n",
      "d_loss_fake:0.0052807386964559555\n",
      "d_loss_wrong:1.086548089981079\n",
      "dis_loss:0.8150999546051025\n",
      "gen_loss:[0.67686474, 0.6696832, 0.0035907538]\n",
      "Batch:31\n",
      "d_loss_real:0.9879341721534729\n",
      "d_loss_fake:0.006392397917807102\n",
      "d_loss_wrong:0.8054665923118591\n",
      "dis_loss:0.6969318389892578\n",
      "gen_loss:[0.6742665, 0.6657998, 0.004233359]\n",
      "Batch:32\n",
      "d_loss_real:1.1044976711273193\n",
      "d_loss_fake:0.014859439805150032\n",
      "d_loss_wrong:0.6851012110710144\n",
      "dis_loss:0.7272389978170395\n",
      "gen_loss:[0.67809063, 0.66683847, 0.0056260726]\n",
      "Batch:33\n",
      "d_loss_real:1.0758378505706787\n",
      "d_loss_fake:0.014223103411495686\n",
      "d_loss_wrong:0.7666330337524414\n",
      "dis_loss:0.7331329584121704\n",
      "gen_loss:[0.6683527, 0.65865004, 0.004851336]\n",
      "Batch:34\n",
      "d_loss_real:1.0702264308929443\n",
      "d_loss_fake:0.011945800855755806\n",
      "d_loss_wrong:0.7383434176445007\n",
      "dis_loss:0.7226855158805847\n",
      "gen_loss:[0.6493014, 0.6392667, 0.005017346]\n",
      "Batch:35\n",
      "d_loss_real:1.003816843032837\n",
      "d_loss_fake:0.0246890839189291\n",
      "d_loss_wrong:0.8284115791320801\n",
      "dis_loss:0.7151835858821869\n",
      "gen_loss:[0.66223496, 0.6544805, 0.0038772274]\n",
      "Batch:36\n",
      "d_loss_real:1.0212252140045166\n",
      "d_loss_fake:0.023086193948984146\n",
      "d_loss_wrong:0.7470985651016235\n",
      "dis_loss:0.7031587958335876\n",
      "gen_loss:[0.6994146, 0.6907375, 0.004338564]\n",
      "Batch:37\n",
      "d_loss_real:1.1509602069854736\n",
      "d_loss_fake:0.027838554233312607\n",
      "d_loss_wrong:0.8020876049995422\n",
      "dis_loss:0.7829616367816925\n",
      "gen_loss:[0.6966701, 0.6869446, 0.004862755]\n",
      "Batch:38\n",
      "d_loss_real:1.0197392702102661\n",
      "d_loss_fake:0.0034727277234196663\n",
      "d_loss_wrong:0.8752281069755554\n",
      "dis_loss:0.7295448482036591\n",
      "gen_loss:[0.69069505, 0.68222964, 0.004232702]\n",
      "Batch:39\n",
      "d_loss_real:1.0907297134399414\n",
      "d_loss_fake:0.03436563163995743\n",
      "d_loss_wrong:0.7108216285705566\n",
      "dis_loss:0.7316616773605347\n",
      "gen_loss:[0.69737333, 0.691558, 0.0029076543]\n",
      "Batch:40\n",
      "d_loss_real:1.0397015810012817\n",
      "d_loss_fake:0.0088307224214077\n",
      "d_loss_wrong:1.0330588817596436\n",
      "dis_loss:0.7803231775760651\n",
      "gen_loss:[0.68764216, 0.6805326, 0.003554801]\n",
      "Batch:41\n",
      "d_loss_real:0.984383225440979\n",
      "d_loss_fake:0.0033406554721295834\n",
      "d_loss_wrong:0.712422788143158\n",
      "dis_loss:0.6711324751377106\n",
      "gen_loss:[0.68499815, 0.6792488, 0.0028746706]\n",
      "Batch:42\n",
      "d_loss_real:1.0626438856124878\n",
      "d_loss_fake:0.00693826749920845\n",
      "d_loss_wrong:0.739869236946106\n",
      "dis_loss:0.7180238217115402\n",
      "gen_loss:[0.6565442, 0.65028894, 0.003127642]\n",
      "Batch:43\n",
      "d_loss_real:0.9218273758888245\n",
      "d_loss_fake:0.007461828179657459\n",
      "d_loss_wrong:0.7388604283332825\n",
      "dis_loss:0.6474942564964294\n",
      "gen_loss:[0.66102076, 0.65226406, 0.004378361]\n",
      "Batch:44\n",
      "d_loss_real:0.9122713804244995\n",
      "d_loss_fake:0.00641672033816576\n",
      "d_loss_wrong:0.7902715802192688\n",
      "dis_loss:0.6553077697753906\n",
      "gen_loss:[0.65189123, 0.6448289, 0.003531161]\n",
      "Batch:45\n",
      "d_loss_real:0.9866700172424316\n",
      "d_loss_fake:0.007721050176769495\n",
      "d_loss_wrong:0.6723709106445312\n",
      "dis_loss:0.6633580029010773\n",
      "gen_loss:[0.63933635, 0.62967277, 0.004831797]\n",
      "Batch:46\n",
      "d_loss_real:0.9864102602005005\n",
      "d_loss_fake:0.0034341271966695786\n",
      "d_loss_wrong:0.7491185069084167\n",
      "dis_loss:0.681343287229538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_loss:[0.6193877, 0.61292183, 0.0032329396]\n",
      "Batch:47\n",
      "d_loss_real:0.9430830478668213\n",
      "d_loss_fake:0.00686442106962204\n",
      "d_loss_wrong:0.7834322452545166\n",
      "dis_loss:0.6691156923770905\n",
      "gen_loss:[0.60192156, 0.5952784, 0.0033215955]\n",
      "Batch:48\n",
      "d_loss_real:1.0062485933303833\n",
      "d_loss_fake:0.00592244416475296\n",
      "d_loss_wrong:0.6947382688522339\n",
      "dis_loss:0.6782894730567932\n",
      "gen_loss:[0.5854961, 0.5803982, 0.0025489647]\n",
      "Batch:49\n",
      "d_loss_real:0.9411125779151917\n",
      "d_loss_fake:0.0023609823547303677\n",
      "d_loss_wrong:0.7266247272491455\n",
      "dis_loss:0.6528027206659317\n",
      "gen_loss:[0.5685433, 0.56137913, 0.0035821027]\n",
      "Batch:50\n",
      "d_loss_real:0.9010886549949646\n",
      "d_loss_fake:0.0021383704151958227\n",
      "d_loss_wrong:0.7380002737045288\n",
      "dis_loss:0.635578989982605\n",
      "gen_loss:[0.54981285, 0.54457355, 0.0026196674]\n",
      "Batch:51\n",
      "d_loss_real:0.9661499261856079\n",
      "d_loss_fake:0.004569024313241243\n",
      "d_loss_wrong:0.7335840463638306\n",
      "dis_loss:0.6676132380962372\n",
      "gen_loss:[0.5392359, 0.5336213, 0.0028073005]\n",
      "Batch:52\n",
      "d_loss_real:0.9192635416984558\n",
      "d_loss_fake:0.0038971351459622383\n",
      "d_loss_wrong:0.7717505097389221\n",
      "dis_loss:0.6535436809062958\n",
      "gen_loss:[0.5274324, 0.52401334, 0.0017095073]\n",
      "Batch:53\n",
      "d_loss_real:0.9685349464416504\n",
      "d_loss_fake:0.0023021260276436806\n",
      "d_loss_wrong:0.7238904237747192\n",
      "dis_loss:0.665815606713295\n",
      "gen_loss:[0.5279302, 0.5224975, 0.002716355]\n",
      "Batch:54\n",
      "d_loss_real:0.8958098888397217\n",
      "d_loss_fake:0.0048583755269646645\n",
      "d_loss_wrong:0.7565239667892456\n",
      "dis_loss:0.6382505297660828\n",
      "gen_loss:[0.5249169, 0.5192827, 0.0028170878]\n",
      "Batch:55\n",
      "d_loss_real:0.9172102212905884\n",
      "d_loss_fake:0.002206302247941494\n",
      "d_loss_wrong:0.7288427352905273\n",
      "dis_loss:0.6413673758506775\n",
      "gen_loss:[0.51526713, 0.509269, 0.0029990543]\n",
      "Batch:56\n",
      "d_loss_real:0.9244836568832397\n",
      "d_loss_fake:0.0025238541420549154\n",
      "d_loss_wrong:0.790438175201416\n",
      "dis_loss:0.660482332110405\n",
      "gen_loss:[0.48882982, 0.48286974, 0.0029800315]\n",
      "Batch:57\n",
      "d_loss_real:0.9765669107437134\n",
      "d_loss_fake:0.001490993658080697\n",
      "d_loss_wrong:0.7163398265838623\n",
      "dis_loss:0.6677411645650864\n",
      "gen_loss:[0.47871196, 0.4725855, 0.0030632252]\n",
      "Batch:58\n",
      "d_loss_real:0.8850496411323547\n",
      "d_loss_fake:0.0064608813263475895\n",
      "d_loss_wrong:0.7605818510055542\n",
      "dis_loss:0.6342855095863342\n",
      "gen_loss:[0.5037995, 0.49842662, 0.002686447]\n",
      "Batch:59\n",
      "d_loss_real:0.9486428499221802\n",
      "d_loss_fake:0.0016136255580931902\n",
      "d_loss_wrong:0.7554611563682556\n",
      "dis_loss:0.6635901182889938\n",
      "gen_loss:[0.5224181, 0.5176143, 0.0024018763]\n",
      "Batch:60\n",
      "d_loss_real:0.9340034127235413\n",
      "d_loss_fake:0.005182155407965183\n",
      "d_loss_wrong:0.7617068290710449\n",
      "dis_loss:0.6587239503860474\n",
      "gen_loss:[0.5011337, 0.4966402, 0.0022467498]\n",
      "Batch:61\n",
      "d_loss_real:1.0022943019866943\n",
      "d_loss_fake:0.0023441496305167675\n",
      "d_loss_wrong:0.731838047504425\n",
      "dis_loss:0.6846926957368851\n",
      "gen_loss:[0.46172085, 0.4577623, 0.0019792777]\n",
      "Batch:62\n",
      "d_loss_real:0.9662909507751465\n",
      "d_loss_fake:0.0017423097742721438\n",
      "d_loss_wrong:0.7079903483390808\n",
      "dis_loss:0.6605786383152008\n",
      "gen_loss:[0.4514342, 0.44822377, 0.0016052085]\n",
      "Batch:63\n",
      "d_loss_real:0.93653404712677\n",
      "d_loss_fake:0.0009962280746549368\n",
      "d_loss_wrong:0.7597069144248962\n",
      "dis_loss:0.658442810177803\n",
      "gen_loss:[0.44370165, 0.43980893, 0.0019463657]\n",
      "Batch:64\n",
      "d_loss_real:0.9534399509429932\n",
      "d_loss_fake:0.0014164354652166367\n",
      "d_loss_wrong:0.7143552899360657\n",
      "dis_loss:0.6556629091501236\n",
      "gen_loss:[0.42063528, 0.41651016, 0.0020625656]\n",
      "Batch:65\n",
      "d_loss_real:0.9665483832359314\n",
      "d_loss_fake:0.0019153407774865627\n",
      "d_loss_wrong:0.7572332620620728\n",
      "dis_loss:0.673061341047287\n",
      "gen_loss:[0.41951275, 0.4157551, 0.0018788322]\n",
      "Batch:66\n",
      "d_loss_real:0.8505455255508423\n",
      "d_loss_fake:0.0015473335515707731\n",
      "d_loss_wrong:0.7585103511810303\n",
      "dis_loss:0.615287184715271\n",
      "gen_loss:[0.41633862, 0.41304713, 0.0016457461]\n",
      "Batch:67\n",
      "d_loss_real:0.9702111482620239\n",
      "d_loss_fake:0.002658834680914879\n",
      "d_loss_wrong:0.6939680576324463\n",
      "dis_loss:0.6592622995376587\n",
      "gen_loss:[0.40550664, 0.40244347, 0.0015315821]\n",
      "Batch:68\n",
      "d_loss_real:0.9431687593460083\n",
      "d_loss_fake:0.0016359493602067232\n",
      "d_loss_wrong:0.7792817950248718\n",
      "dis_loss:0.6668138206005096\n",
      "gen_loss:[0.39701143, 0.3936566, 0.0016774107]\n",
      "Batch:69\n",
      "d_loss_real:0.9457638263702393\n",
      "d_loss_fake:0.0013304174644872546\n",
      "d_loss_wrong:0.997136116027832\n",
      "dis_loss:0.7224985510110855\n",
      "gen_loss:[0.40803358, 0.40497512, 0.0015292272]\n",
      "Batch:70\n",
      "d_loss_real:0.8857649564743042\n",
      "d_loss_fake:0.0011258148588240147\n",
      "d_loss_wrong:0.7389262318611145\n",
      "dis_loss:0.6278954893350601\n",
      "gen_loss:[0.39861125, 0.39540762, 0.0016018142]\n",
      "Batch:71\n",
      "d_loss_real:0.9645345211029053\n",
      "d_loss_fake:0.001076504006050527\n",
      "d_loss_wrong:0.6500539779663086\n",
      "dis_loss:0.6450498849153519\n",
      "gen_loss:[0.3819486, 0.37811792, 0.0019153398]\n",
      "Batch:72\n",
      "d_loss_real:1.0033501386642456\n",
      "d_loss_fake:0.001363118994049728\n",
      "d_loss_wrong:0.6679567098617554\n",
      "dis_loss:0.6690050214529037\n",
      "gen_loss:[0.36999848, 0.36726677, 0.00136585]\n",
      "Batch:73\n",
      "d_loss_real:0.878943681716919\n",
      "d_loss_fake:0.001735839992761612\n",
      "d_loss_wrong:0.7310873866081238\n",
      "dis_loss:0.6226776540279388\n",
      "gen_loss:[0.3634993, 0.36034712, 0.0015760966]\n",
      "Batch:74\n",
      "d_loss_real:0.9345906376838684\n",
      "d_loss_fake:0.0013244971632957458\n",
      "d_loss_wrong:0.7201416492462158\n",
      "dis_loss:0.6476618498563766\n",
      "gen_loss:[0.3596743, 0.3563573, 0.0016585065]\n",
      "Batch:75\n",
      "d_loss_real:0.9472597241401672\n",
      "d_loss_fake:0.0018191782291978598\n",
      "d_loss_wrong:0.7195683121681213\n",
      "dis_loss:0.6539767384529114\n",
      "gen_loss:[0.36896268, 0.36443788, 0.0022624]\n",
      "Batch:76\n",
      "d_loss_real:0.9178442358970642\n",
      "d_loss_fake:0.00470652524381876\n",
      "d_loss_wrong:0.6905639171600342\n",
      "dis_loss:0.6327397227287292\n",
      "gen_loss:[0.36395505, 0.36106226, 0.0014463922]\n",
      "Batch:77\n",
      "d_loss_real:0.896711528301239\n",
      "d_loss_fake:0.0011449122102931142\n",
      "d_loss_wrong:0.740994393825531\n",
      "dis_loss:0.6338905841112137\n",
      "gen_loss:[0.36716533, 0.36485374, 0.0011557941]\n",
      "Batch:78\n",
      "d_loss_real:0.8905389308929443\n",
      "d_loss_fake:0.001459543826058507\n",
      "d_loss_wrong:0.7293516993522644\n",
      "dis_loss:0.627972275018692\n",
      "gen_loss:[0.36859888, 0.365713, 0.0014429372]\n",
      "Batch:79\n",
      "d_loss_real:0.895925760269165\n",
      "d_loss_fake:0.0009027213673107326\n",
      "d_loss_wrong:0.6973005533218384\n",
      "dis_loss:0.6225136965513229\n",
      "gen_loss:[0.3693001, 0.36640596, 0.0014470681]\n",
      "Batch:80\n",
      "d_loss_real:0.8809936046600342\n",
      "d_loss_fake:0.0013294888194650412\n",
      "d_loss_wrong:0.7025683522224426\n",
      "dis_loss:0.6164712607860565\n",
      "gen_loss:[0.3632703, 0.3601359, 0.0015671935]\n",
      "Batch:81\n",
      "d_loss_real:0.8966439366340637\n",
      "d_loss_fake:0.0014171188231557608\n",
      "d_loss_wrong:0.671286940574646\n",
      "dis_loss:0.6164979785680771\n",
      "gen_loss:[0.36129627, 0.3581129, 0.0015916764]\n",
      "Batch:82\n",
      "d_loss_real:0.942344069480896\n",
      "d_loss_fake:0.0017692588735371828\n",
      "d_loss_wrong:0.7355780601501465\n",
      "dis_loss:0.6555088609457016\n",
      "gen_loss:[0.35751486, 0.3549197, 0.0012975712]\n",
      "Batch:83\n",
      "d_loss_real:0.8601475954055786\n",
      "d_loss_fake:0.001612797612324357\n",
      "d_loss_wrong:0.6913670897483826\n",
      "dis_loss:0.6033187657594681\n",
      "gen_loss:[0.35198206, 0.3493694, 0.0013063205]\n",
      "Batch:84\n",
      "d_loss_real:0.8739986419677734\n",
      "d_loss_fake:0.0014247192302718759\n",
      "d_loss_wrong:0.738106369972229\n",
      "dis_loss:0.6218820959329605\n",
      "gen_loss:[0.3508953, 0.34798998, 0.0014526583]\n",
      "Batch:85\n",
      "d_loss_real:0.8912719488143921\n",
      "d_loss_fake:0.0013123061507940292\n",
      "d_loss_wrong:0.7003380656242371\n",
      "dis_loss:0.6210485696792603\n",
      "gen_loss:[0.3450222, 0.3429315, 0.0010453414]\n",
      "Batch:86\n",
      "d_loss_real:0.9071296453475952\n",
      "d_loss_fake:0.0011569119524210691\n",
      "d_loss_wrong:0.6845579743385315\n",
      "dis_loss:0.6249935477972031\n",
      "gen_loss:[0.34213322, 0.33978504, 0.0011740986]\n",
      "Batch:87\n",
      "d_loss_real:0.9102022051811218\n",
      "d_loss_fake:0.001301881275139749\n",
      "d_loss_wrong:0.6809988021850586\n",
      "dis_loss:0.6256762742996216\n",
      "gen_loss:[0.34197065, 0.33882993, 0.0015703525]\n",
      "Batch:88\n",
      "d_loss_real:0.9384000301361084\n",
      "d_loss_fake:0.0012802156852558255\n",
      "d_loss_wrong:0.7051293253898621\n",
      "dis_loss:0.6458023935556412\n",
      "gen_loss:[0.34095854, 0.33796525, 0.0014966407]\n",
      "Batch:89\n",
      "d_loss_real:0.9220288991928101\n",
      "d_loss_fake:0.0012746993452310562\n",
      "d_loss_wrong:0.8125789761543274\n",
      "dis_loss:0.6644778698682785\n",
      "gen_loss:[0.34430683, 0.34002918, 0.0021388181]\n",
      "Batch:90\n",
      "d_loss_real:0.8683009147644043\n",
      "d_loss_fake:0.0023774360306560993\n",
      "d_loss_wrong:0.762933075428009\n",
      "dis_loss:0.6254780888557434\n",
      "gen_loss:[0.3439458, 0.34031683, 0.0018144784]\n",
      "Batch:91\n",
      "d_loss_real:0.9092873334884644\n",
      "d_loss_fake:0.0005381909431889653\n",
      "d_loss_wrong:0.7276128530502319\n",
      "dis_loss:0.6366814225912094\n",
      "gen_loss:[0.3423129, 0.33929065, 0.0015111222]\n",
      "Batch:92\n",
      "d_loss_real:0.9528906941413879\n",
      "d_loss_fake:0.0013176469365134835\n",
      "d_loss_wrong:0.8260773420333862\n",
      "dis_loss:0.6832940876483917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_loss:[0.34673932, 0.34196448, 0.002387424]\n",
      "Batch:93\n",
      "d_loss_real:0.9000836610794067\n",
      "d_loss_fake:0.0026609902270138264\n",
      "d_loss_wrong:0.6926270723342896\n",
      "dis_loss:0.6238638460636139\n",
      "gen_loss:[0.34655923, 0.3418108, 0.0023742109]\n",
      "Batch:94\n",
      "d_loss_real:0.9262113571166992\n",
      "d_loss_fake:0.0016714726807549596\n",
      "d_loss_wrong:0.743886411190033\n",
      "dis_loss:0.6494951546192169\n",
      "gen_loss:[0.34360677, 0.33908254, 0.0022621192]\n",
      "Batch:95\n",
      "d_loss_real:0.9273353815078735\n",
      "d_loss_fake:0.00029826629906892776\n",
      "d_loss_wrong:0.6813615560531616\n",
      "dis_loss:0.6340826451778412\n",
      "gen_loss:[0.34233847, 0.33871335, 0.0018125563]\n",
      "Batch:96\n",
      "d_loss_real:0.8706289529800415\n",
      "d_loss_fake:0.0007702867733314633\n",
      "d_loss_wrong:0.6971432566642761\n",
      "dis_loss:0.6097928583621979\n",
      "gen_loss:[0.33736345, 0.33348912, 0.0019371586]\n",
      "Batch:97\n",
      "d_loss_real:0.8980609774589539\n",
      "d_loss_fake:0.000473544467240572\n",
      "d_loss_wrong:0.6858587265014648\n",
      "dis_loss:0.6206135600805283\n",
      "gen_loss:[0.33524662, 0.330934, 0.0021563224]\n",
      "Batch:98\n",
      "d_loss_real:0.8923650979995728\n",
      "d_loss_fake:0.0006975088035687804\n",
      "d_loss_wrong:0.7251990437507629\n",
      "dis_loss:0.6276566833257675\n",
      "gen_loss:[0.33391932, 0.33049864, 0.0017103374]\n",
      "Batch:99\n",
      "d_loss_real:0.9121782779693604\n",
      "d_loss_fake:0.0007715515675954521\n",
      "d_loss_wrong:0.6713472604751587\n",
      "dis_loss:0.624118834733963\n",
      "gen_loss:[0.331702, 0.32774404, 0.0019789846]\n",
      "Batch:100\n",
      "d_loss_real:0.8961056470870972\n",
      "d_loss_fake:0.000821922323666513\n",
      "d_loss_wrong:0.6745601296424866\n",
      "dis_loss:0.6168983429670334\n",
      "gen_loss:[0.3314459, 0.32865056, 0.0013976669]\n",
      "Batch:101\n",
      "d_loss_real:0.8741182088851929\n",
      "d_loss_fake:0.0006663561216555536\n",
      "d_loss_wrong:0.7430543899536133\n",
      "dis_loss:0.622989296913147\n",
      "gen_loss:[0.33014157, 0.32801062, 0.0010654759]\n",
      "Batch:102\n",
      "d_loss_real:0.9394264221191406\n",
      "d_loss_fake:0.0009755892679095268\n",
      "d_loss_wrong:0.6962662935256958\n",
      "dis_loss:0.6440236866474152\n",
      "gen_loss:[0.32926324, 0.32675588, 0.0012536758]\n",
      "Batch:103\n",
      "d_loss_real:0.9283779859542847\n",
      "d_loss_fake:0.001186144771054387\n",
      "d_loss_wrong:0.6781258583068848\n",
      "dis_loss:0.6340169906616211\n",
      "gen_loss:[0.33159152, 0.3284952, 0.0015481515]\n",
      "Batch:104\n",
      "d_loss_real:0.9023042321205139\n",
      "d_loss_fake:0.001374745974317193\n",
      "d_loss_wrong:0.7618904709815979\n",
      "dis_loss:0.6419684141874313\n",
      "gen_loss:[0.3293219, 0.32660598, 0.0013579604]\n",
      "Batch:105\n",
      "d_loss_real:0.9005293846130371\n",
      "d_loss_fake:0.0020378767512738705\n",
      "d_loss_wrong:0.7470212578773499\n",
      "dis_loss:0.6375294774770737\n",
      "gen_loss:[0.330712, 0.32851624, 0.0010978797]\n",
      "Batch:106\n",
      "d_loss_real:0.8570406436920166\n",
      "d_loss_fake:0.0008392892777919769\n",
      "d_loss_wrong:0.7089139223098755\n",
      "dis_loss:0.6059586256742477\n",
      "gen_loss:[0.33017498, 0.3280731, 0.0010509301]\n",
      "Batch:107\n",
      "d_loss_real:0.9296592473983765\n",
      "d_loss_fake:0.0019665968138724566\n",
      "d_loss_wrong:0.6995907425880432\n",
      "dis_loss:0.6402189582586288\n",
      "gen_loss:[0.32984373, 0.32736814, 0.0012377881]\n",
      "Batch:108\n",
      "d_loss_real:0.9042574167251587\n",
      "d_loss_fake:0.0012579511385411024\n",
      "d_loss_wrong:0.6879635453224182\n",
      "dis_loss:0.62443408370018\n",
      "gen_loss:[0.3287921, 0.32669955, 0.0010462662]\n",
      "Batch:109\n",
      "d_loss_real:0.9179893732070923\n",
      "d_loss_fake:0.0011576386168599129\n",
      "d_loss_wrong:0.6989976167678833\n",
      "dis_loss:0.6340335011482239\n",
      "gen_loss:[0.32871965, 0.32615083, 0.0012844085]\n",
      "Batch:110\n",
      "d_loss_real:0.8538601398468018\n",
      "d_loss_fake:0.001006417442113161\n",
      "d_loss_wrong:0.701562762260437\n",
      "dis_loss:0.6025723665952682\n",
      "gen_loss:[0.32954082, 0.32683682, 0.0013519903]\n",
      "Batch:111\n",
      "d_loss_real:0.8675378561019897\n",
      "d_loss_fake:0.0009003948071040213\n",
      "d_loss_wrong:0.6942278146743774\n",
      "dis_loss:0.6075509786605835\n",
      "gen_loss:[0.33002162, 0.3268237, 0.001598947]\n",
      "Batch:112\n",
      "d_loss_real:0.925839900970459\n",
      "d_loss_fake:0.0008371311705559492\n",
      "d_loss_wrong:0.6982238292694092\n",
      "dis_loss:0.6376851946115494\n",
      "gen_loss:[0.32882023, 0.3256964, 0.0015619077]\n",
      "Batch:113\n",
      "d_loss_real:0.8843261003494263\n",
      "d_loss_fake:0.0004291530349291861\n",
      "d_loss_wrong:0.6969664692878723\n",
      "dis_loss:0.6165119558572769\n",
      "gen_loss:[0.32959083, 0.32661366, 0.0014885778]\n",
      "Batch:114\n",
      "d_loss_real:0.8555968403816223\n",
      "d_loss_fake:0.0006003680173307657\n",
      "d_loss_wrong:0.6761313676834106\n",
      "dis_loss:0.5969813615083694\n",
      "gen_loss:[0.3292525, 0.32648498, 0.0013837612]\n",
      "Batch:115\n",
      "d_loss_real:0.8692830801010132\n",
      "d_loss_fake:0.0008316613966599107\n",
      "d_loss_wrong:0.7042173147201538\n",
      "dis_loss:0.6109037846326828\n",
      "gen_loss:[0.32935372, 0.32639232, 0.0014806988]\n",
      "Batch:116\n",
      "d_loss_real:0.9198266267776489\n",
      "d_loss_fake:0.0005814721807837486\n",
      "d_loss_wrong:0.7107921242713928\n",
      "dis_loss:0.6377567052841187\n",
      "gen_loss:[0.32948348, 0.32579044, 0.0018465263]\n",
      "Batch:117\n",
      "d_loss_real:0.8787856101989746\n",
      "d_loss_fake:0.00045730266720056534\n",
      "d_loss_wrong:0.7284173369407654\n",
      "dis_loss:0.6216114610433578\n",
      "gen_loss:[0.32830858, 0.32622957, 0.001039511]\n",
      "Batch:118\n",
      "d_loss_real:0.8752399682998657\n",
      "d_loss_fake:0.0007931956206448376\n",
      "d_loss_wrong:0.6799985766410828\n",
      "dis_loss:0.6078179329633713\n",
      "gen_loss:[0.32878074, 0.32617575, 0.0013024944]\n",
      "Batch:119\n",
      "d_loss_real:0.8816800117492676\n",
      "d_loss_fake:0.00039416231447830796\n",
      "d_loss_wrong:0.6941573023796082\n",
      "dis_loss:0.6144778728485107\n",
      "gen_loss:[0.3283666, 0.32628882, 0.0010388875]\n",
      "Batch:120\n",
      "d_loss_real:0.8713715076446533\n",
      "d_loss_fake:0.000697561539709568\n",
      "d_loss_wrong:0.6951432228088379\n",
      "dis_loss:0.6096459478139877\n",
      "gen_loss:[0.32860437, 0.3261312, 0.00123659]\n",
      "Batch:121\n",
      "d_loss_real:0.9007325172424316\n",
      "d_loss_fake:0.0007756470004096627\n",
      "d_loss_wrong:0.7159737944602966\n",
      "dis_loss:0.6295536160469055\n",
      "gen_loss:[0.32786542, 0.32558477, 0.0011403278]\n",
      "Batch:122\n",
      "d_loss_real:0.8544933199882507\n",
      "d_loss_fake:0.0006067107897251844\n",
      "d_loss_wrong:0.6907720565795898\n",
      "dis_loss:0.600091353058815\n",
      "gen_loss:[0.32870308, 0.32610667, 0.0012982097]\n",
      "Batch:123\n",
      "d_loss_real:0.9697023630142212\n",
      "d_loss_fake:0.0008476547664031386\n",
      "d_loss_wrong:0.7349634170532227\n",
      "dis_loss:0.668803945183754\n",
      "gen_loss:[0.32729572, 0.3252941, 0.0010008048]\n",
      "Batch:124\n",
      "d_loss_real:0.9201869368553162\n",
      "d_loss_fake:0.0009296018397435546\n",
      "d_loss_wrong:0.6589251160621643\n",
      "dis_loss:0.6250571459531784\n",
      "gen_loss:[0.3273584, 0.325334, 0.0010121894]\n",
      "Batch:125\n",
      "d_loss_real:0.8282343149185181\n",
      "d_loss_fake:0.0007103619282133877\n",
      "d_loss_wrong:0.7117508053779602\n",
      "dis_loss:0.5922324508428574\n",
      "gen_loss:[0.3283237, 0.3261277, 0.0010979961]\n",
      "Batch:126\n",
      "d_loss_real:0.8683182001113892\n",
      "d_loss_fake:0.00041058327769860625\n",
      "d_loss_wrong:0.6912303566932678\n",
      "dis_loss:0.6070693284273148\n",
      "gen_loss:[0.32861346, 0.3263746, 0.0011194373]\n",
      "Batch:127\n",
      "d_loss_real:0.9435755610466003\n",
      "d_loss_fake:0.0009302502730861306\n",
      "d_loss_wrong:0.6597647666931152\n",
      "dis_loss:0.6369615346193314\n",
      "gen_loss:[0.32762223, 0.32544476, 0.0010887384]\n",
      "Batch:128\n",
      "d_loss_real:0.8517300486564636\n",
      "d_loss_fake:0.00044325864291749895\n",
      "d_loss_wrong:0.7778477668762207\n",
      "dis_loss:0.6204377859830856\n",
      "gen_loss:[0.32972148, 0.32645112, 0.0016351817]\n",
      "Batch:129\n",
      "d_loss_real:0.8924968242645264\n",
      "d_loss_fake:0.0004247669712640345\n",
      "d_loss_wrong:0.6893478035926819\n",
      "dis_loss:0.618691548705101\n",
      "gen_loss:[0.32902396, 0.3257221, 0.0016509301]\n",
      "Batch:130\n",
      "d_loss_real:0.8819974064826965\n",
      "d_loss_fake:0.00038921349914744496\n",
      "d_loss_wrong:0.706403911113739\n",
      "dis_loss:0.6176969856023788\n",
      "gen_loss:[0.3278064, 0.32561564, 0.0010953825]\n",
      "Batch:131\n",
      "d_loss_real:0.9001723527908325\n",
      "d_loss_fake:0.000454172637546435\n",
      "d_loss_wrong:0.7072095274925232\n",
      "dis_loss:0.6270021051168442\n",
      "gen_loss:[0.32819262, 0.3253186, 0.001437003]\n",
      "Batch:132\n",
      "d_loss_real:0.8698353171348572\n",
      "d_loss_fake:0.0006730181630700827\n",
      "d_loss_wrong:0.6881839632987976\n",
      "dis_loss:0.6071318984031677\n",
      "gen_loss:[0.3277854, 0.32541245, 0.001186471]\n",
      "Batch:133\n",
      "d_loss_real:0.8707476854324341\n",
      "d_loss_fake:0.0005973270162940025\n",
      "d_loss_wrong:0.6973935961723328\n",
      "dis_loss:0.6098715662956238\n",
      "gen_loss:[0.3275612, 0.32537967, 0.0010907685]\n",
      "Batch:134\n",
      "d_loss_real:0.8693534135818481\n",
      "d_loss_fake:0.0004769255465362221\n",
      "d_loss_wrong:0.6840598583221436\n",
      "dis_loss:0.6058108955621719\n",
      "gen_loss:[0.32752568, 0.32539177, 0.0010669485]\n",
      "Batch:135\n",
      "d_loss_real:0.8718027472496033\n",
      "d_loss_fake:0.00037749126204289496\n",
      "d_loss_wrong:0.6880335211753845\n",
      "dis_loss:0.6080041229724884\n",
      "gen_loss:[0.32720038, 0.32556027, 0.0008200493]\n",
      "Batch:136\n",
      "d_loss_real:0.8745392560958862\n",
      "d_loss_fake:0.0004959547077305615\n",
      "d_loss_wrong:0.691119909286499\n",
      "dis_loss:0.6101735979318619\n",
      "gen_loss:[0.32719934, 0.32549948, 0.0008499317]\n",
      "Batch:137\n",
      "d_loss_real:0.8826231956481934\n",
      "d_loss_fake:0.0004932606825605035\n",
      "d_loss_wrong:0.6735585331916809\n",
      "dis_loss:0.6098245531320572\n",
      "gen_loss:[0.32740766, 0.32548803, 0.0009598145]\n",
      "Batch:138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_loss_real:0.8562018871307373\n",
      "d_loss_fake:0.0004988958826288581\n",
      "d_loss_wrong:0.6918932795524597\n",
      "dis_loss:0.6011989861726761\n",
      "gen_loss:[0.32746857, 0.32544303, 0.0010127723]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (64, 64, 64, 3) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-03ef90cee121>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Save images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0msave_rgb_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/Users/alexnails/Desktop/StackGAN/results/gen_{}_{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# Save models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-03ef90cee121>\u001b[0m in \u001b[0;36msave_rgb_img\u001b[0;34m(image, path)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nmep/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nmep/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nmep/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nmep/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/nmep/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (64, 64, 64, 3) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmep",
   "language": "python",
   "name": "nmep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
